{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Der folgende Beitrag soll die zentralen Elemente und Konzepte von TensorFlow näher erläutern und anhand eines Praxisbeispiels verdeutlichen. Der Fokus liegt dabei nicht auf der formalen mathematischen Darstellung der Funktionsweise neuronaler Netze, sondern auf den grundlegenden Konzepten und Terminologien von TensorFlow sowie deren Umsetzung in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhalte\n",
    "\n",
    "> #### 1) Theoretische Grundlagen\n",
    "    - Installation\n",
    "    - Grundlegende Funktionen von TensorFlow\n",
    "> #### 2) Anwendungsbeispiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Theoretische Grundlagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow ist aktuell eines der wichtigsten Frameworks zur Programmierung von neuronalen Netzen, Deep Learning Modellen und anderen Machine Learning Algorithmen. Es basiert auf C++, das jedoch über eine Python Library gesteuert wird. TensorFlow wurde von Google entwickelt und wurde der Öffentlichkeit im November 2015 als Open-Source Framework zur Verfügung gestellt. Es lässt sich sowohl auf CPU als auch auf GPU (Clustern) ausführen. Seit kurzem existiert auch ein R Package, mit dem TensorFlow genutzt werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Die Installation erfolgt über die Anaconda Umgebung unter \"Environments\" -> \"Not installed\" -> \"tensorflow\" oder über die Konsole (Anconda Prompt) mit dem Befehl `pip install --upgrade tensorflow` für die CPU-Version, da wir in dieser Ausarbeitung nur die CPU rechnen lassen werden. Es kann auch die GPU-Version mit installiert werden, dazu gibt es eine separate Anleitung bei Tensorflow: <a href=\"https://www.tensorflow.org/install/\">Link</a>. Jetzt Testen wir noch, ob es funktioniert hat. Dazu den folgenden Befehl ausführen und in der Ausgabe sollte **'Hallo, Welt!'** erscheinen:\n",
    "> Hinweis: Die Befehle von TensorFlow werden weiter unten erklärt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hallo, Welt!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant(\"Hallo, Welt!\")\n",
    "with tf.Session() as sess:\n",
    "    erg = sess.run(hello)\n",
    "print(erg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlegende Funktionen von TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronale Netze\n",
    "Ein neuronales Netz ist eine beliebige Anzahl miteinander verbundener Neuronen, die als Teil eines Nervensystems einen Zusammenhang bilden, der einer bestimmten Funktion dienen soll. <br>\n",
    "Das Künstliche Neuronale Netz (KNN) ist bis zu einem gewissen Grad dem Aufbau des biologischen Gehirns nachempfunden. Es besteht aus einem abstrahierten Modell miteinander verbundener Neuronen, durch deren spezielle Anordnung und Verknüpfung sich Anwendungsprobleme aus verschiedenen Bereichen wie der Statistik, der Technik oder der Wirtschaftswissenschaften computerbasiert lösen lassen. Das Neuronale Netz ist ein Forschungsgegenstand der Neuroinformatik und Teilgebiet der Künstlichen Intelligenz. Neuronale Netze müssen, bevor sie Problemstellungen lösen können, trainiert werden.\n",
    "\n",
    "**Ganz triviales Bespiel:** Wir möchten jetzt ein neuronales Netz erstellen, welches beispielsweise die Sinusfunktion nachahmt. Mal angenommen wir haben bestimmte Datensätze gegeben, an sich bekommen wir bei einer Funktion ein **X-Wert** rein, die berechnet irgendetwas und danach bekommen wir ein **Y-Wert** wieder raus. In unserem Beispiel hier wird die Höhe berechnet. Wenn in einer Sinusfunktion (f(x)=sin(x)), X=0 reinkommt, kommt Y=0 raus.\n",
    "\n",
    "<img src=\"img/bild_1.png\" width=\"600\">\n",
    "\n",
    "Mal angenommen wir haben irgendwelche Werte vorliegen (rote Punkte). Wir möchten jetzt, dass unser neuronales Netz diese Sinusfunktion so gut wie nachahmt. Wir haben die Lern- und Testdaten und darüber hinaus haben wir eine \"Input\" Schicht (erste Schicht). Am Ende haben wir unser \"Output\"-Schicht. Zwischen den beiden Schichten ist noch ein Hidden Layer (verdeckte Schicht), diese Schicht können wir beim erstellen sehen aber von außerhalb sind diese nicht betrachtbar (darauf gehen wir im nächsten Abschnitt noch näher ein)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Graph\n",
    "Die grundlegende Funktionsweise von TensorFlow basiert auf einem sog. Graphen. Dieser bezeichnet eine abstrakte Darstellung des zugrundeliegenden mathematischen Problems in Form eines gerichteten Diagramms. Das Diagramm besteht aus Knoten und Kanten die miteinander verbunden sind. Die **Knoten** des Graphen repräsentieren in TensorFlow mathematische Operationen und die **Kanten** repräsentieren die Daten, die durch das System fließen. Durch die richtige Verbindung der Knoten kann ein Graph erstellt werden, der die notwendigen Daten und mathematischen Operationen zur Erstellung eines neuronalen Netzes beinhaltet. Für unser neuronales Netz, welches wir später am Anwendungsbeispiel erstellen werden, soll das folgende Bild die grundlegende Funktionsweise verdeutlichen:\n",
    "\n",
    "<img src=\"img/bild_2.png\" width=\"600\">\n",
    "\n",
    "Wir überlegen uns, welche Dimension hat unser Input, was haben wir als Input gegeben (meistens sind das Vektoren) und welche Dimension hat unser Output? Wir fangen bei `X1` an zu zählen und je nachdem welche Dimension wir haben, werden wir auch dementsprechend mehr Inputs (X2, X3, .., Xn) haben. In der obenstehenden Abbildung bekommen wir z.B. \"nur\" ein Input-Wert, z.B. eine Zahl (X-Koordinaten der roten Punkte der ersten Abbildung), rein. Diese Zahl kann als **Platzhalter**, in Tensorflow \"placeholder\" genannt (mehr im Abschnitt: Platzhalter), verstanden werden. Durch die Zahl möchten wir ein Output-Wert rausbekommen (`Y`). Zwischen **Input und Output Layer ist der Hidden Layer**, welche 3 Neuronen (Knoten) besitzt. An den Kanten stehen verschiedene **Gewichte** (W1-W6, meistens eine Matrix). Anhand dieser Gewichte fließt der Input-Wert in die Hidden Layer und an dieser Stelle werden mathematischen Operationen und Summen durchgeführt. Alle Zahlen, die als Input hereinkommen, werden nach dem gleichen Ablauf verarbeitet. Diese abstrakte Darstellung der durchzuführenden mathematischen Operationen ist der Kern von TensorFlow. <br>\n",
    "Natürlich handelt es sich bei diesem Beispiel um eine triviale Operation. Die benötigten Schritte und Berechnungen in neuronalen Netzen sind deutlich komplexer. Die prinzipielle Funktionsweise der graphenbasierten Ausführung bleibt jedoch bestehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bild_3.png\" width=\"600\">\n",
    "\n",
    "Meistens wird noch ein **Bias** hinzugefügt (`X0`). Der Bias hat den **Wert 1**. Das entscheidende hier sind die Gewichte zu den Neuronen, die alle aufsummiert werden. Die Summe des Neurons Z1 ist beispielsweise (`W1*X1 + W7*X0`). Auf die Summe wird dann eine Aktivierungsfunktion angewandt (darüber später mehr, da es sehr viele verschiedene Aktivierungsfunktionen gibt) und wird anschließend in die nächste Schicht weitergegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Netz:\n",
    "<img src=\"img/bild_4.png\" width=\"700\">\n",
    "\n",
    "Die obenstehende Abbildung soll die Architektur des Netzwerkes schematisch illustrieren. Das Modell besteht aus drei Teilen: 1. dem Input Layer, 2. dem Hidden Layer sowie 3. dem Output Layer. Diese Architektur nennt man **Feedforward Netz**. Feedforward beschreibt die Tatsache, dass die Daten nur in eine Richtung durch das Netzwerk fließen. Andere Arten von neuronalen Netzen und Deep Learning Modellen beinhalten Architekturen, die es den Daten erlauben sich auch \"rückwärts\" oder in Schleifen im Netzwerk zu bewegen (z.B das Rekurrentes neuronales Netz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platzhalter\n",
    "Platzhalter spielen in TensorFlow eine zentrale Rolle. Platzhalter beinhalten in der Regel alle Daten, die zum Training des neuronalen Netzes benötigt werden. Hierbei handelt es sich normalerweise um Inputs (die Eingangssignale des Modells) und Outputs (die zu vorhersagenden Variablen). Die Platzhalter werden mittels `tf.placeholder()` definiert. Da TensorFlow auf einem C++ Backend basiert, müssen die Datentypen der Platzhalter im Voraus **fix definiert und können nicht zur Laufzeit angepasst werden**. Dies geschieht innerhalb der Funktion `tf.placeholder()`. Am unteren Codebeispiel sind `X` und `Y` Platzhalter. Das erste Argument innerhalb der Klammer gibt den Datentyp an, das zweite die Form bzw. die Dimension der Tensoren an, die in den Platzhaltern gespeichert werden. Die Größe `[None, 10]` bedeutet, dass es jeweils 10 Inputs gibt (10 aufgereiht in einem einzelnen Vektor) und der Parameter `None` weist TensorFlow an, diese Dimension flexibel zu halten, da im aktuellen Stadium noch unklar ist, welche Ausdehnung die Daten zum Trainieren des Modells haben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [None, 10])\n",
    "Y = tf.placeholder('float') #Eindimensionaler Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variablen\n",
    "Neben Platzhaltern sind Variablen ein weiteres Kernkonzept der Funktionsweise von TensorFlow. Während Platzhalter zum Speichern der Input- und Output-Daten verwendet werden, sind Variablen **flexibel und können Ihre Werte während der Laufzeit der Berechnung verändern**. Der wichtigste Anwendungsbereich für Variablen in neuronalen Netzen sind die Gewichtungsmatrizen der Neuronen (`Weights`) und Biasvektoren (`Biases`), die während des Trainings stetig an die Daten angepasst werden. Im folgenden Codeblock werden die Variablen für ein einschichtiges, Feedforward Netz definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der zu Inputs und Outputs \n",
    "inputs = 10\n",
    "outputs = 1\n",
    "\n",
    "# Anzahl der Neuronen \n",
    "neuronen = 64\n",
    "\n",
    "# Hidden Layer: Variablen für Weights und Biases \n",
    "hidden_layer = {'weights': tf.Variable(tf.random_normal([inputs, neuronen])),\n",
    "                'biases': tf.Variable(tf.random_normal([neuronen]))}\n",
    "\n",
    "output_layer = {'weights': tf.Variable(tf.random_normal([neuronen, outputs])),\n",
    "                'biases': tf.Variable(tf.random_normal([outputs]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Beispielcode werden 10 Inputs und ein Output definiert. Die Anzahl der Neuronen im Hidden Layer beträgt 64. Im nächsten Schritt werden die benötigten Variablen instanziiert. Für ein einfaches Feedforward Netz werden zunächst die Gewichtungsmatrizen und Biaswerte zwischen Input- und Hidden Layer benötigt. Diese werden in dem Objekt `hidden_layer={}` mittels der Funktion `tf.Variable()` angelegt. Innerhalb von `tf.Variable()` wird die Funktion `tf.random_normal()` verwendet, auf die wir im nächsten Abschnitt genauer eingehen. \n",
    "\n",
    "Nach der Definition der benötigten Variablen zwischen Input- und Hidden Layer werden noch die Weights und Biases zwischen Hidden- und Output Layer instanziiert.\n",
    "Es ist wichtig zu verstehen, welche Dimensionen die benötigten Matrizen der Weights und Biases annehmen müssen, damit sie korrekt verarbeitet werden. Als Daumenregel für Gewichtungsmatrizen in einfachen Feedforward Netzen gilt, dass die zweite Dimension des vorhergehenden Layers die erste Dimension des aktuellen Layers darstellt. <br>\n",
    "Was sich zunächst sehr komplex anhört ist schlussendlich nichts anderes als das Weiterreichen von Outputs von Layer zur Layer im Netz. Die Dimension der Biaswerte entspricht normalerweise der Anzahl der Neuronen im aktuellen Layer. Im obigen Beispiel wird somit aus der Anzahl Inputs und der Anzahl Neuronen eine Gewichtungsmatrix der Form `[inputs, neuronen] = [10, 64]` sowie ein Biasvektor im Hidden Layer der Ausdehnung `[hidden_layer] = [64]`. Zwischen dem Hidden- und Output Layer hat die Gewichtungsmatrix die Form `[neuronen, outputs] = [64, 1]` sowie der Biasvektor die Form `[1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisierung\n",
    "Bei der Definition der Variablen im Codeblock des vorhergehenden Abschnitts wurde die Funktionen `'weight'` und `'biases'` verwendet. Die Art und Weise, wie die initialen Gewichtungsmatrizen und Biasvektoren gefüllt werden, hat einen **großen Einfluss darauf, wie schnell und wie gut sich das Modell an die vorliegenden Daten anpassen kann**. Dies hängt damit zusammen, dass neuronale Netze und Deep Learning Modelle mittels numerischer Optimierungsverfahren trainiert werden, die immer von einer bestimmten Startposition aus beginnen die Parameter des Modells anzupassen. Wenn nun eine vorteilhafte Startposition für das Training des neuronalen Netzes gewählt wird, wirkt sich dies in der Regel positiv auf die Rechenzeit und Anpassungsgüte des Modells aus.\n",
    "\n",
    "In TensorFlow sind verschiedenste Initialisierungsstrategien implementiert. Angefangen von einfachen Matrizen mit immer dem gleichen Wert, z.B. `tf.zeros()` über Zufallswerte, z.B. `tf.random_normal()` oder `tf.truncated_normal()` bis hin zu komplexeren Funktionen wie `tf.glorot_normal()` oder `tf.variance_scaling()`. Je nachdem, welche Initialisierung der Gewichte und Biaswerte vorgenommen wird, kann das Ergebnis des Modelltrainings mehr oder weniger stark variieren.\n",
    "\n",
    "<img src=\"img/bild_5.png\" width=\"700\">\n",
    "\n",
    "Im obigen Beipsiel sind unsere **Fehler** auf der Y-Achsenabschnitt dargestellt. Auf der X-Achse ist eine **Kante**, also die Gewichte z.B. W1. Die **Kurve zeigt unsere Gewichts-Werte**. Danach nehmen wir ganz **zufällig** einen Wert z.B. roter Punkt.<br> \n",
    "Die Frage lautet nun: Wie lernt unser Netzwerk? Da gibt es beispielsweise den **Gradienten Abstieg**. Ein Gradient ist eine Ableitung von einer Funktion mit mehreren Variablen (dazu mehr im Abschnitt: Kostenfunktion und Optimierer). Der rote Punkt wird abgeleitet (mit bestimmten mathematischen Regeln, die wir hier nicht näher eingehen werden) und sehen dann mit dem Gradienten Abstieg die Richtung zum nächsten Minimum. D.h. wir bewegen uns in die Richtung bis wir im **lokalen Minima** gelandet sind. Dadurch haben wir hier den Fehler von 100 auf 90 verringert. <br>\n",
    "Aber wir kommen jetzt aus dem Minimum nicht mehr weg, da wir durch nochmaliges Ableitung keine Richtung durch den Vektor mehr gezeigt bekommen. Deshalb wird jetzt das Ganze mehrmals mit verschiedenen zufälligen Gewichts-Werten (pinker und blauer Punkt) wiederholt. Umso mehr wir hier diesen Lösungsraum durchsuchen, also mit **verschiedenen Werten starten**, umso mehr erreichen wir bessere lokale Minima, um möglicherweise den **globalen Minima** zu finden. Globale Minima ist der tiefste Punkt einer Funktion (gelbe Markierung). Dadurch haben wir den Fehler für diesen einen Gewichts-Wert (Kante) minimiert. Das heißt alle anderen Gewichte werden nach dem gleichen Ablauf verarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer: Variablen für Weights und Biases \n",
    "hidden_layer = {'weights': tf.Variable(tf.truncated_normal([inputs, neuronen], stddev=0.1)),\n",
    "                'biases': tf.Variable(tf.truncated_normal([neuronen], stddev=0.1))}\n",
    "\n",
    "output_layer = {'weights': tf.Variable(tf.truncated_normal([neuronen, outputs], stddev=0.1)),\n",
    "                'biases': tf.Variable(tf.truncated_normal([outputs], stddev=0.1))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im obigen Codebeispiel verwenden wir `tf.truncated_normal()` statt wie davor `tf.random_normal()` als Initialisierungsstrategie für die Gewichte und Biaswerte. Dabei steht `stddev` für die Standardabweichung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design der Netzwerkarchitektur\n",
    "Nach der Implementierung der benötigten Gewichtungs- und Biasvariablen wird im nächsten Schritt die Netzwerkarchitektur, auch Topologie genannt, erstellt. Hierbei werden sowohl Platzhalter als auch Variablen in Form von **aufeinanderfolgenden Matrizenmultiplikationen miteinander kombiniert**.\n",
    "\n",
    "Weiterhin werden bei der Spezifikation der Topologie auch die **Aktivierungsfunktionen** der Neuronen festgelegt. Aktivierungsfunktionen führen eine **nichtlineare Transformation der Outputs** der Hidden Layer durch, bevor diese an den nächsten Layer weitergegeben werden. Dadurch wird das gesamte System nichtlinear und kann sich dadurch sowohl an lineare als auch nichtlineare Funktionen anpassen. Es existieren zahllose Aktivierungsfunktionen für neuronale Netze, die sich im Laufe der Zeit entwickelt haben (z.B. `sigmoid`). Heutiger Standard bei der Entwicklung von Deep Learning Modellen ist die sog. **Rectified Linear Unit** (`ReLU`), die sich in vielen Anwendungen als vorteilhaft herausgestellt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer Summe\n",
    "hidden_sum = tf.add(tf.matmul(X, hidden_layer['weights']), hidden_layer['biases'])\n",
    "hidden_sum = tf.nn.relu(hidden_sum)\n",
    "\n",
    "# Output Layer Summer\n",
    "output_sum = tf.add(tf.matmul(hidden_sum, output_layer['weights']), output_layer['biases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU Aktivierungsfunktionen sind in TensorFlow mittels `tf.nn.relu()` implementiert. Die Aktivierungsfunktion nimmt den Output der Matrizenmultiplikation zwischen Platzhalter und Gewichtungsmatrix sowie der Addition der Biaswerte entgegen und transformiert diese. Das Ergebnis der nichtlinearen Transformation wird als Output an den nächsten Layer weitergegeben, der diesen als Input einer erneuten Matrizenmultiplikation verwendet. Da es sich beim zweiten Layer bereits um den Output Layer handelt, wird in diesem Beispiel keine erneute ReLU Transformation durchgeführt.\n",
    "\n",
    "<img src=\"img/bild_4.png\" width=\"700\">\n",
    "Die obenstehende Abbildung soll die Architektur des Netzwerkes schematisch illustrieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostenfunktion\n",
    "Die Kostenfunktion (Cost Function) des neuronalen Netzes wird verwendet, um eine Maßzahl zur Bestimmung der Abweichung zwischen der **Prognose** des Modells und den **tatsächlich beobachteten Daten** zu berechnen. Hierzu stehen, je nachdem, ob es sich um eine Klassifikation oder Regression handelt, verschiedene Kostenfunktionen zur Verfügung. Für **Klassifikationen** wird heute zumeist die sog. **Kreuzentropie** (Cross Entropy) verwendet und für **Regressionen** die **mittlere quadratische Abweichung** (Mean Squared Error, **MSE**). Grundsätzlich kann jede mathematisch differenzierbare Funktion als Kostenfunktion verwendet werden.\n",
    "\n",
    "<img src=\"img/bild_6.png\" width=\"700\">\n",
    "\n",
    "Im obigen Beispiel wird die mittlere quadratische Abweichung als Kostenfunktion bzw. Fehlerfunktion verwendet. Die Sinusfunktion, die wir nachahmen wollen, ist hier dargestellt. Bei dem X-Wert (roter Punkt) sollte beispielsweise eine 1 rauskommen, unser neuronales Netzt sagt aber es ist eine 0,5. Somit wird der Fehler wie folgt berechnet: 1 - 0,5 = 0,5². D.h. der Fehler für diesen einen Wert beträgt 0,25. Und je nachdem wie viele Punkte angeschaut werden, wird der Mittelwert gebildet. Wenn unser neuronales Netz gerade keine guten vorhersagen trifft, könnte es z.B. wie die blauen Punkte aussehen, also denkt er, dass es eine Gerade ist, obwohl wir eine Sinuskurve haben. D.h. er muss sich von **Generation zu Generation so antrainieren, die Gewichte und Biaswerte so zu verändern**, dass er eventuell auf die Idee kommt, sich der Sinuskurve anzunähern. Hier kommt auch der Optimierer ins Spiel (nächster Abschnitt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function für Regressionen\n",
    "mse = tf.reduce_mean(tf.squared_difference(output_sum, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die mittlere quadratische Abweichung stehen in TensorFlow die beiden Funktionen `tf.reduce_mean()` sowie `tf.squared_difference()` zur Verfügung, die einfach miteinander kombiniert werden können. Man sieht, dass die Funktionsargumente von `tf.squared_difference()` zum einen der Platzhalter `Y` ist, der die tatsächlich beobachteten Outputs enthält sowie das Objekt `output_sum`, das die vom Modell erzeugten Prognosen beinhaltet. An der Kostenfunktion laufen also die tatsächlich beobachteten Daten mit den Modellprognosen zusammen und werden miteinander verglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierer\n",
    "Der Optimierer (Optimizer) hat die Aufgabe, auf Basis der berechneten Modellabweichungen der Kostenfunktion, **die Gewichte und Biaswerte des Netzes während des Trainings anzupassen**. Um dies zu tun, werden von TensorFlow sog. Gradienten der Kostenfunktion berechnet, die die Richtung anzeigen, in der die Gewichte und Biaswerte angepasst werden müssen, um die Kostenfunktion bzw. Fehlerfunktion des Modells zu **minimieren**. Die Entwicklung von schnellen und stabilen Optimierern ist ein großer Forschungszweig im Bereich neuronaler Netze und Deep Learning.\n",
    "\n",
    "<img src=\"img/bild_7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "optimizer = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der sog. `tf.AdamOptimizer()` verwendet, der im Moment einer der am häufigsten angewendeten Optimierer ist. Adam steht für Adaptive moment estimation und ist eine methodische Kombination von zwei anderen Optimierungstechniken (AdaGrad und RMSProp). An dieser Stelle gehen wir nicht auf die mathematischen Details der Optimierer ein, da dies den Scope dieser Einführung deutlich sprengen würde. Wichtig zu verstehen ist, dass es verschiedene Optimierer gibt, die auf unterschiedlichen Strategien zur Berechnung der benötigten Anpassungen der Gewichtungs- und Biaswerte basieren. \n",
    "\n",
    "> Verschiedene Optimierer sind z.B.: \n",
    "> - AdadeltaOptimizer() \n",
    "> - AdagradDAOptimizer() \n",
    "> - AdagradOptimizer()\n",
    "> - ProximalAdagradOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session\n",
    "Die TensorFlow **Session** ist das Grundgerüst zur Ausführung des **Graphen**. Die Session wird mit dem Kommando `tf.Session()` gestartet. Bevor die Session nicht gestartet wurde, kann keine Berechnung innerhalb des Graphen erfolgen. Die Methode `global_variables_initializer()` initialisert die Variablen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session starten \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem Codebeispiel wird eine TensorFlow Session in dem Objekt `sess` instanziiert und kann im Folgenden an einer beliebigen Stelle im Graphen ausgeführt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Nachdem die notwendigen Bestandteile des neuronalen Netzes definiert wurden, können diese nun im Rahmen des Modelltrainings miteinander verbunden werden. Das Training von neuronalen Netzen läuft heute in der Regel über ein sog. \"Minibatch Training\" ab. Minibatch bedeutet, dass wiederholt zufällige Stichproben der Inputs und Outputs verwendet werden, um die Gewichtungen und Biaswerte des Netzes anzupassen. Hierzu wird ein Parameter definiert, der die Größe der Zufallsstichprobe (Batches) der Daten steuert. Hierbei ist es häufig so, dass die Daten ohne Zurücklegen gezogen werden, sodass jede Beobachtung im Datensatz für eine **Trainingsrunde** (auch Epoche genannt) nur einmal dem Netz präsentiert wird. D.h. eine Epoche entspricht ein Trainingsdurchlauf. Die Anzahl der Epochen wird ebenfalls als Parameter durch den Anwender definiert und können beliebig verändert werden, um eventuell bessere Ergebnisse zu erzielen. <br>\n",
    "Die einzelnen **Batches** werden über die zuvor erstellten Platzhalter `X` und `Y` mittels **Feed Dictionary** (TensorFlow Terminologie) in den TensorFlow Graphen übergeben und entsprechend im Modells verwendet. Dies geschieht in Kombination mit der zuvor definierten `Session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktuellen Batch in Netzwerk übergeben\n",
    "batch_size = 100\n",
    "epoche = 30\n",
    "\n",
    "epoche_x = <datensatz>.train.next_batch(batch_size)\n",
    "epoche_y = <datensatz>.train.next_batch(batch_size)\n",
    "\n",
    "sess.run(optimizer, feed_dict={X: epoche_x, Y: epoche_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurze Zusammenfassung: Im obigen Beispiel wird der Optimierungsschritt `optimizer` im Graphen durchgeführt. Damit TensorFlow die notwendigen Berechnungen ausführen kann, müssen über das Argument `feed_dict={}` Daten in den Graphen übergeben werden, die für die Berechnungen an die Stelle der Platzhalter `X` und `Y` treten sollen.<br>\n",
    "Nach der Übergabe mittels `feed_dict={}` wird `X` über die Multiplikation mit der Gewichtungsmatrix zwischen Input und Hidden Layer in das Netz eingespeist und durch die Aktivierungsfunktion nichtlinear transformiert. Anschließend wird das Ergebnis des Hidden Layers wiederum mit der Gewichtungsmatrix zwischen Hidden und Output Layer multipliziert und an den Output Layer weitergereicht. Hier wird durch die Kostenfunktion die Differenz zwischen der Prognose des Netzes und den tatsächlich beobachteten Werten `Y` berechnet. Auf Basis des Optimierers werden nun für jeden einzelnen Gewichtungsparameter im Netz die Gradienten berechnet. Die Gradienten wiederum sind die Basis auf denen eine Anpassung der Gewichtungen in Richtung der Minimierung der Kostenfunktion durchgeführt wird. Diesen Vorgang nennt man auch Gradient Descent. Anschließend wird der soeben beschriebene Prozess mit dem nächsten Batch erneut durchgeführt. Das neuronale Netz bewegt sich also in jeder Iteration näher in Richtung der Kostenminimierung, sprich in Richtung einer kleineren Abweichung zwischen Prognose und beobachteten Werten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Anwendungsbeispiel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierfür verwenden wir ein Datensatz namens MNIST (Mixed National Institute of Standards and Technology) <a href=\"http://yann.lecun.com/exdb/mnist/\">Link</a>. MNIST ist eine große Datenbank, die handgeschriebene Ziffern enthält und häufig zum Trainieren von neuronalen Netzen für Bildverarbeitungssysteme verwendet wird. Sie enthält 60 000 Lerndaten und 10 000 Testdaten, die alle jeweils 28x28 Pixel groß sind. \n",
    "\n",
    "<img src=\"img/mnist_bild.png\" width=\"500\">\n",
    "**Quelle:** eBook - Einführung in TensorFlow: Deep-Learning-Systeme programmieren, trainieren, skalieren und deployen von Thom Hoff, Yehezkel S. Resheff, Itay Lieder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir importieren Tensorflow wie gewohnt und mit dem Befehl `from tensorflow.examples.tutorials.mnist` benutzen wir externe Daten. Statt den MNIST-Datensatz herunterzuladen und in unser Programm einzulesen, verwenden wir eine eingebaute Hilfsfunktion, um den Datensatz bei Bedarf zu holen. Solche Ladefunktionen gibt es für die meisten häufig verwendeten Datensätze. Der Zweite Import lädt die Hilfsfunktion, die wir dann sowohl nutzen werden, um die Daten für uns herunterzuladen, als auch um sie zu verwalten und nach Bedarf aufzuteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode `read_data_sets()` des MNIST-Laders lädt den Datensatz herunter und speichert ihn lokal, sodass er später im Programm verwendet werden kann. Das erste Argument ist der Ort, an dem wir die Daten lokal gespeichert haben möchten. Das zweite Argument sagt dem Lader, wie wir die Daten markiert haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben in diesem Anwendungsbeispiel 10 Output-Klassen (10 Ziffern). Die Funktion `one_hot=True` sorgt dafür, dass nur bei der Ziffer die erkannt wird eine 1 und bei den anderen eine 0 steht. Also eine binäre True-False-Geschichte. Jedes Index hier ist ein Output.\n",
    "\n",
    "<img src=\"img/bild_8.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt überlegen wir uns, wie groß unser neuronales Netz sein wird, d.h. wie viele Layers und wie viele Neuronen in den Layers haben wir? Dabei können wir beliebig frei experimentieren und verschiedene Werte testen. Wir entscheiden uns vorerst für zwei hidden layer. Dabei verwenden wir Variablen, um die Anzahl der Neuronen bzw. Knoten (hier z.B. 300, 150..) leicht anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_anzahl = 784\n",
    "verdeckte_schicht_knoten_1 = 300\n",
    "verdeckte_schicht_knoten_2 = 150\n",
    "output_schicht_knoten = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weil wir 10 Ziffern haben, setzen wir unser Output gleich 10. Die Batch Größe kann wieder beliebig gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_anzahl = 10\n",
    "batch_groesse = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren die Placeholder. Da die MNIST-Bilder jeweils 28x28 Pixel groß sind, setzen wir für unseren Input Placeholder die Größe 784 (28*28= 784). Jeder Pixel ist für uns ein Input (d.h. wir haben auf dem Input Layer 784 \"Kreise\" wenn man sich das grafisch vorstellen würde)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, inputs_anzahl])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren die Layer. Als Initialisierungstrategie nehmen wir `tf.random_normal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schicht = {'weights': tf.Variable(tf.random_normal([inputs_anzahl,verdeckte_schicht_knoten_1])),\n",
    "                 'biases': tf.Variable(tf.random_normal([verdeckte_schicht_knoten_1]))}\n",
    "\n",
    "verdeckte_schicht_1 = {'weights': tf.Variable(tf.random_normal([verdeckte_schicht_knoten_1, verdeckte_schicht_knoten_2])),\n",
    "                        'biases': tf.Variable(tf.random_normal([verdeckte_schicht_knoten_2]))}\n",
    "\n",
    "verdeckte_schicht_2 = {'weights': tf.Variable(tf.random_normal([verdeckte_schicht_knoten_2, output_schicht_knoten])),\n",
    "                        'biases': tf.Variable(tf.random_normal([output_schicht_knoten]))}\n",
    "\n",
    "output_schicht = {'weights': tf.Variable(tf.random_normal([output_schicht_knoten, outputs_anzahl])),\n",
    "                  'biases': tf.Variable(tf.random_normal([outputs_anzahl]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Implementierung der benötigten Gewichtungs- und Biasvariablen erstellen wir im nächsten Schritt die Netzwerkarchitektur. Hierbei werden sowohl Platzhalter als auch Variablen in Form von aufeinanderfolgenden Matrizenmultiplikationen miteinander kombiniert. Als Aktivierungsfunktion nehmen wir `ReLu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schicht_summe = tf.add(tf.matmul(x, input_schicht['weights']), input_schicht['biases'])\n",
    "input_schicht_summe = tf.nn.relu(input_schicht_summe)\n",
    "\n",
    "verdeckte_schicht_1_summe = tf.add(tf.matmul(input_schicht_summe, verdeckte_schicht_1['weights']), verdeckte_schicht_1['biases'])\n",
    "verdeckte_schicht_1_summe = tf.nn.relu(verdeckte_schicht_1_summe)\n",
    "\n",
    "verdeckte_schicht_2_summe = tf.add(tf.matmul(verdeckte_schicht_1_summe, verdeckte_schicht_2['weights']), verdeckte_schicht_2['biases'])\n",
    "verdeckte_schicht_2_summe = tf.nn.relu(verdeckte_schicht_2_summe)\n",
    "\n",
    "output_schicht_summe = tf.add(tf.matmul(verdeckte_schicht_2_summe, output_schicht['weights']), output_schicht['biases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt möchten wir unser Netzwerk trainieren. Um das Modell anzulernen, verwenden wir nur die Lerndaten, und bei der Bewertung prüfen wir die Genauigkeit anhand der Testdaten. Da wir in unserem Anwendungsbeispiel eine Klassifikation vorliegen haben, werden wir die Kreuzentropie (Cross Entropy) als Kostenfunktion verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vorhersage = output_schicht_summe\n",
    "fehler = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vorhersage, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt brauchen wir noch ein Optimierungsverfahren. Als Optimierer verwenden wir `AdamOptimizer()`. Außerdem möchten wir definieren, wie viele Trainingsrunden unser Programm durchlaufen soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimierer = tf.train.AdamOptimizer().minimize(fehler)\n",
    "epochen = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir starten die Session und initialisieren alle Variablen. Mit der For-Schleife und `epoche_fehler=0` schauen wir, wie viele Fehler wir pro Epoche haben. `epoche_x` und `epoche_y` stehen für den Input- und Output-Wert in dem jeweiligen Example. Danach berechnen wir die Fehler (kosten). Die \"entgangene\" Kosten werden jedesmal zu `epoche_fehler` hinzuaddiert. Schließlich werden unsere Fehler in der jeweiligen Iteration der Epoche ausgegeben. Zum Schluss wird die Genauigkeit berechnet und anschließend wird es auch ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche 0 von 10 Fehleranzahl: 532502.3523025513\n",
      "Epoche 1 von 10 Fehleranzahl: 113641.12822341919\n",
      "Epoche 2 von 10 Fehleranzahl: 68722.44838953018\n",
      "Epoche 3 von 10 Fehleranzahl: 46821.03289651871\n",
      "Epoche 4 von 10 Fehleranzahl: 33993.71790468693\n",
      "Epoche 5 von 10 Fehleranzahl: 25326.312127053738\n",
      "Epoche 6 von 10 Fehleranzahl: 18697.15370734036\n",
      "Epoche 7 von 10 Fehleranzahl: 13836.97631019657\n",
      "Epoche 8 von 10 Fehleranzahl: 10644.390171372152\n",
      "Epoche 9 von 10 Fehleranzahl: 8153.900210417332\n",
      "Genauigkeit: 0.9338\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoche in range(epochen):\n",
    "        epoche_fehler = 0\n",
    "        for _ in range(int(mnist.train.num_examples / batch_groesse)):\n",
    "            epoche_x, epoche_y = mnist.train.next_batch(batch_groesse)\n",
    "            _, kosten = sess.run([optimierer, fehler], feed_dict={x: epoche_x, y: epoche_y})\n",
    "            epoche_fehler += kosten\n",
    "                \n",
    "        print('Epoche', epoche, 'von', epochen, 'Fehleranzahl:', epoche_fehler)\n",
    "\n",
    "    richtiges_ergebnis = tf.equal(tf.argmax(vorhersage, 1), tf.argmax(y, 1))\n",
    "        \n",
    "    genauigkeit = tf.reduce_mean(tf.cast(richtiges_ergebnis, 'float'))\n",
    "        \n",
    "    print('Genauigkeit:', genauigkeit.eval({x: mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quellen:\n",
    "- https://www.tensorflow.org/tutorials/\n",
    "- https://www.statworx.com/de/blog/data-science/einfuehrung-tensorflow/\n",
    "- https://www.bigdata-insider.de/was-ist-ein-neuronales-netz-a-686185/\n",
    "- Ebook: Einführung in TensorFlow: Deep-Learning-Systeme programmieren, trainieren, skalieren und deployen von Thom Hoff, Yehezkel S. Resheff, Itay Lieder\n",
    "- Ebook: TensorFlow Machine Learning Cookbook von Nick McClure\n",
    "- Free Ebook: Getting Started With Tensorflow von Giancarlo Zaccone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
