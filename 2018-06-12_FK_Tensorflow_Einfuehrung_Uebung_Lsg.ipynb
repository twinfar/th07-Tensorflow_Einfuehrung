{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung zu TensorFlow Einführung - Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Theorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1:\n",
    "Was zeichnet ein **Feedforward Netz** aus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: Feedforward beschreibt die Tatsache, dass die Daten nur in eine Richtung durch das Netzwerk fließen. <br>\n",
    "(Andere Arten von neuronalen Netzen und Deep Learning Modellen beinhalten Architekturen, die es den Daten erlauben sich auch \"rückwärts\" oder in Schleifen im Netzwerk zu bewegen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2:\n",
    "Was ist der Unterschied zwischen **Platzhaltern** und **Variablen** in TensorFlow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: Die Datentypen der Platzhalter müssen im Voraus fix definiert und können nicht zur Laufzeit angepasst werden. Variablen sind dagegen flexibel und können Ihre Werte während der Laufzeit der Berechnung verändern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3:\n",
    "Wieso sind die **Gewichtungsmatrizen** und **Biasvektoren** so wichtig?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: Weil die Art und Weise, wie die initialen Gewichtungsmatrizen und Biasvektoren gefüllt werden, <!--einen großen Einfluss darauf hat,-->hat einen großen Einfluss darauf, wie schnell und wie gut sich das Modell an die vorliegenden Daten anpassen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4:\n",
    "Was machen **Aktivierungsfunktionen**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: Aktivierungsfunktionen führen eine nichtlineare Transformation der Outputs der Hidden Layer durch bevor diese an den nächsten Layer weitergegeben werden. Dadurch wird das gesamte System nichtlinear und kann sich dadurch sowohl an lineare als auch nichtlineare Funktionen anpassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5:\n",
    "Wofür wird die **Kostenfunktion** verwendet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: Um eine Maßzahl zur Bestimmung der Abweichung zwischen der Prognose des Modells und den tatsächlich beobachteten Daten zu berechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 6:\n",
    "Welche Aufgabe hat der **Optimierer**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwort: Der Optimierer hat die Aufgabe, auf Basis der berechneten Modellabweichungen der Kostenfunktion, die Gewichte und Biaswerte des Netzes während des Trainings anzupassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1:\n",
    "Installieren und importieren Sie TensorFlow und testen Sie die Ausgabe **\"Hallo, Welt!**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hallo, Welt!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant(\"Hallo, Welt!\")\n",
    "with tf.Session() as sess:\n",
    "    erg = sess.run(hello)\n",
    "print(erg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2:\n",
    "Lese Sie den MNIST-Datensatz extern ein und benutzen Sie die eingebaute Hilfsfunktion, um den Datensatz bei Bedarf zu holen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3:\n",
    "Versuchen Sie mit Tensorflow eine Genauigkeit von mindestens 96% zu erreichen. \n",
    "> Hinweis:\n",
    "> - Sie können die Anzahl der Neuronen verändern.\n",
    "> - Sie können verschiedene Initialisierungstrategien testen (z.B. `tf.truncated_normal()`).\n",
    "> - Sie können die Anzahl der Hidden Layer verändern\n",
    "> - Sie können verschiedene Kostenfunktionen testen.\n",
    "> - Sie können die Batch Größe verändern.\n",
    "> - Sie können die Anzahl der Epochen verändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche 0 von 30 Fehleranzahl: 19.4333662558347\n",
      "Epoche 1 von 30 Fehleranzahl: 5.2465693755075336\n",
      "Epoche 2 von 30 Fehleranzahl: 3.354029950685799\n",
      "Epoche 3 von 30 Fehleranzahl: 2.399135280866176\n",
      "Epoche 4 von 30 Fehleranzahl: 1.7854389373678714\n",
      "Epoche 5 von 30 Fehleranzahl: 1.35845065000467\n",
      "Epoche 6 von 30 Fehleranzahl: 1.045437863911502\n",
      "Epoche 7 von 30 Fehleranzahl: 0.8112868081079796\n",
      "Epoche 8 von 30 Fehleranzahl: 0.6438511332380585\n",
      "Epoche 9 von 30 Fehleranzahl: 0.5054265198414214\n",
      "Epoche 10 von 30 Fehleranzahl: 0.40276321503915824\n",
      "Epoche 11 von 30 Fehleranzahl: 0.3734016614034772\n",
      "Epoche 12 von 30 Fehleranzahl: 0.2866677249985514\n",
      "Epoche 13 von 30 Fehleranzahl: 0.2482088070246391\n",
      "Epoche 14 von 30 Fehleranzahl: 0.2972177489864407\n",
      "Epoche 15 von 30 Fehleranzahl: 0.36083575544762425\n",
      "Epoche 16 von 30 Fehleranzahl: 0.2740551330643939\n",
      "Epoche 17 von 30 Fehleranzahl: 0.22521702147787437\n",
      "Epoche 18 von 30 Fehleranzahl: 0.21798393670178484\n",
      "Epoche 19 von 30 Fehleranzahl: 0.197368223380181\n",
      "Epoche 20 von 30 Fehleranzahl: 0.16636146372184157\n",
      "Epoche 21 von 30 Fehleranzahl: 0.18926155639928766\n",
      "Epoche 22 von 30 Fehleranzahl: 0.18800795058632502\n",
      "Epoche 23 von 30 Fehleranzahl: 0.13920910023443867\n",
      "Epoche 24 von 30 Fehleranzahl: 0.09475559313432314\n",
      "Epoche 25 von 30 Fehleranzahl: 0.20739951574068982\n",
      "Epoche 26 von 30 Fehleranzahl: 0.12919716060787323\n",
      "Epoche 27 von 30 Fehleranzahl: 0.19170068199309753\n",
      "Epoche 28 von 30 Fehleranzahl: 0.15836890621721977\n",
      "Epoche 29 von 30 Fehleranzahl: 0.12519232203703723\n",
      "Genauigkeit: 0.9809\n"
     ]
    }
   ],
   "source": [
    "inputs_anzahl = 784\n",
    "outputs_anzahl = 10\n",
    "\n",
    "# Hidden Layer: z.B. die Anzahl an Neuronen ändern\n",
    "versteckte_schicht_knoten_1 = 400\n",
    "versteckte_schicht_knoten_2 = 400\n",
    "output_schicht_knoten = 300\n",
    "\n",
    "# z.B. die Batch Größe ändern\n",
    "batch_groesse = 150\n",
    "\n",
    "x = tf.placeholder('float', [None, inputs_anzahl])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "# z.B. Initialisierungstrategie ändern statt tf.random_normal() -> tf.truncated_normal() mit Standardabweichung\n",
    "input_schicht = {'weights': tf.Variable(tf.truncated_normal([inputs_anzahl,versteckte_schicht_knoten_1], stddev=0.1)),\n",
    "                 'biases': tf.Variable(tf.truncated_normal([versteckte_schicht_knoten_1], stddev=0.1))}\n",
    "\n",
    "versteckte_schicht_1 = {'weights': tf.Variable(tf.truncated_normal([versteckte_schicht_knoten_1, versteckte_schicht_knoten_2], stddev=0.1)),\n",
    "                        'biases': tf.Variable(tf.truncated_normal([versteckte_schicht_knoten_2], stddev=0.1))}\n",
    "\n",
    "versteckte_schicht_2 = {'weights': tf.Variable(tf.truncated_normal([versteckte_schicht_knoten_2, output_schicht_knoten], stddev=0.1)),\n",
    "                        'biases': tf.Variable(tf.truncated_normal([output_schicht_knoten], stddev=0.1))}\n",
    "\n",
    "output_schicht = {'weights': tf.Variable(tf.truncated_normal([output_schicht_knoten, outputs_anzahl], stddev=0.1)),\n",
    "                  'biases': tf.Variable(tf.truncated_normal([outputs_anzahl], stddev=0.1))}\n",
    "\n",
    "\n",
    "input_schicht_summe = tf.add(tf.matmul(x, input_schicht['weights']), input_schicht['biases'])\n",
    "input_schicht_summe = tf.nn.relu(input_schicht_summe)\n",
    "\n",
    "versteckte_schicht_1_summe = tf.add(tf.matmul(input_schicht_summe, versteckte_schicht_1['weights']), versteckte_schicht_1['biases'])\n",
    "versteckte_schicht_1_summe = tf.nn.relu(versteckte_schicht_1_summe)\n",
    "\n",
    "versteckte_schicht_2_summe = tf.add(tf.matmul(versteckte_schicht_1_summe, versteckte_schicht_2['weights']), versteckte_schicht_2['biases'])\n",
    "versteckte_schicht_2_summe = tf.nn.relu(versteckte_schicht_2_summe)\n",
    "\n",
    "output_schicht_summe = tf.add(tf.matmul(versteckte_schicht_2_summe, output_schicht['weights']), output_schicht['biases'])\n",
    "\n",
    "\n",
    "vorhersage = output_schicht_summe\n",
    "\n",
    "# z.B. Kostenfunktion bzw. Fehlerfunktion verändern\n",
    "#fehler = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vorhersage, labels=y))\n",
    "fehler = tf.reduce_mean(tf.squared_difference(vorhersage, y))\n",
    "\n",
    "# Optimierer\n",
    "optimierer = tf.train.AdamOptimizer().minimize(fehler)\n",
    "\n",
    "# z.B. Trainingsdurchläufe ändern\n",
    "epochen = 30\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoche in range(epochen):\n",
    "        epoche_fehler = 0\n",
    "        for _ in range(int(mnist.train.num_examples / batch_groesse)):\n",
    "            epoche_x, epoche_y = mnist.train.next_batch(batch_groesse)\n",
    "            _, kosten = sess.run([optimierer, fehler], feed_dict={x: epoche_x, y: epoche_y})\n",
    "            epoche_fehler += kosten\n",
    "                \n",
    "        print('Epoche', epoche, 'von', epochen, 'Fehleranzahl:', epoche_fehler)\n",
    "\n",
    "    richtiges_ergebnis = tf.equal(tf.argmax(vorhersage, 1), tf.argmax(y, 1))\n",
    "        \n",
    "    genauigkeit = tf.reduce_mean(tf.cast(richtiges_ergebnis, 'float'))\n",
    "        \n",
    "    print('Genauigkeit:', genauigkeit.eval({x: mnist.test.images, y:mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
